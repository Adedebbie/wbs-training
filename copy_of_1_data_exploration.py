# -*- coding: utf-8 -*-
"""Copy of 1-data-exploration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b2oXml7k9_sKmMl2sIwIkjSRuE7SsEtQ

# Pandas DataFrames

Pandas DataFrames are mutable two-dimensional structures of data with labeled axes where: 
* each row represents a different observation
* each column represents a different variable

As always, we first need to import the Pandas module:
"""

import pandas as pd

"""Next, if we want a DataFrame with 5 rows and 2 columns, we can do it from a [dictionary](https://www.w3schools.com/python/python_dictionaries.asp), a [list](https://www.w3schools.com/python/python_lists.asp) of lists, a list of dictionaries, etc.

We are going to create a 5 row, 2 column DataFrame from a dictionary. To do this, we will follow these steps:

1. Create a dictionary where the keys will be the names of the columns and the values will be lists, with as many elements as the number of rows we want.
2. Convert that dictionary to dataframe with pandas' `pd.DataFrame()` function.
"""

# 1. Create the dictionary
data = {
    "x":[1, 2, 3, 4, 5], 
    "y":[6, 7, 8, 9, 10]
}

data['x']

# 2. Convert dictionary to dataframe
df = pd.DataFrame(data)
print(df)
# df

"""As we said, we have created a dataframe with 5 rows and two columns, called x and y respectively.

**Observation**: As a result of `print()`, we have not only obtained the 5 rows and 2 columns, but there is an additional "column" of 5 numbers ordered vertically from 0 to 4. This column is called the **index** and it is simply the name of each row, which by default is their ordinal position: 0 indicates the first row; the 1, the second; and so on.

## Import a csv file to dataframe

Most of the time, you will not be creating dataframes yourself, but importing (or "reading") data from a csv file or a database into a pandas dataframe. It's easy to do with pandas' read functions:
"""

url = 'https://drive.google.com/file/d/1FYhN_2AzTBFuWcfHaRuKcuCE6CWXsWtG/view?usp=sharing' # orderlines.csv
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
df = pd.read_csv(path)

"""## Dataframe dimensions

With the `.shape` attribute we can calculate the dimensions (number of rows and columns) of the dataframe.
"""

df.shape

"""As a result we obtain a [tuple](https://www.w3schools.com/python/python_tuples.asp) where the first element is the number of rows, which in our case is 293983, while the second element is the number of columns, which in our example was 7."""

nrows = df.shape[0]
ncols = df.shape[1]
print("The number of rows is", nrows)
print("The number of columns is", ncols)

"""`DataFrame.size` returns the total number of values that the dataframe has (the number of rows multiplied by the number of columns):"""

df.size

# check if that's true
df.shape[0] * df.shape[1] == df.size

"""With the `.ndim` attribute we calculate the number of dimensions that the dataframe has. This will always be 2, as it consists of rows and columns."""

df.ndim

"""## Dataframes exploration

The `DataFrame.head()` and `DataFrame.tail()` methods are used to display the first or last rows of the dataframe. Looking at the raw data is a great way to get a grasp of what's in there. By default, 5 rows will be shown, but you can change that:
"""

df.head()

df.head(9)

df.tail()

"""The methods `DataFrame.info()`, `DataFrame.describe()` and `DataFrame.nunique()` give a general overview of what's in the dataframe:"""

df.info()

df.describe()

df.nunique()

"""The `.unique()` method returns the unique values from a column as a numpy array, which can be indexed with `[]`:"""

df['sku'].unique()[:10]

"""The `isna()` method returns a boolean for each value: `True` if that value is "missing" (which is represented as `NaN` in numpy and pandas) and `False` if the value is not missing:"""

df.isna()

"""We can then use `DataFrame.sum()` to add up all these booleans for each column, and count how many missing values are there in the dataframe, since `True` is interpreted as `1` and `False` as `0`:"""

df.isna().sum()

"""`DataFrame.duplicated()` also returns a boolean output, but in this case just one value per row: `True` if that row is duplicated and `False` if it's not. Again, using `sum()` allows us to count how many `True` values (i.e. duplicated rows) are there in total:"""

df.duplicated().sum() # parameters keep=False
# df.drop_duplicates()

"""`DataFrame.nlargest(n, columns)` will return the top `n` rows with the largest value for whatever column we specify in `columns`. Below, we see the rows with the largest product quantity values:"""

df.nlargest(5, 'product_quantity')

""" `DataFrame.nsmallest()` does the same, for the smallest values:"""

df.nsmallest(5, 'product_quantity')

"""So far, we have seen two ways to explore DataFrames:

* `.shape`, `.size` and `.ndim`, among others, are **atributes**. They are written without parentheses and give you raw "metadata" about the DataFrame you are calling them on.
* `.head()`, `.describe()` and `.isna()`, among others, are **methods**. They are written with parentheses and perform some sort of calculation, transformation or aggregation. A method is like a function that is tied to a specific object type.

DataFrames have a lot of attributes and methods and may not be obvious whether something belongs to one type or the other. Whenever in doubt, check [the documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html?highlight=dataframe#pandas.DataFrame). If you scroll down past the list of examples you will find two sections listing all the attributes and methods.

## Columns

Given a DataFrame, we can select a particular column in several ways:

* Indicating the name of the column between square brackets, `[]`
* With the `.loc[]` attribute (by name or tag)
* With the `.iloc[]` attribute (by position)

Plain square brackets `[]` are used to just view a column if you know its name and **don't want to modify it**:
"""

# select the column by name
df['id_order']

"""`.loc[]` takes two arguments: `[rows, columns]`. Passing `:` to the rows argument means "grabbing all the rows", which allows you to select a whole column if you know its name. This method is more flexible as you will see in the future, and allows you to modify the data."""

# method .loc[]
df.loc[:, 'id_order']

"""`iloc[]` works similarly, but only accepts integers (which represent the positions of the rows and columns):"""

# method .iloc[]
print(df.iloc[:, 0])

"""### Select multiple columns

If we want to select more than one column, we can do it with all the options listed above, with slight modifications in some cases:
"""

# note that we pass a list inside of the []
df[['id_order','sku']]

# .loc()
df.loc[:, ["id_order", "sku"]]

"""With `.loc[]` and `:` you can select all columns between two columns you specify."""

# .loc()
df.loc[:, "id_order":"sku"]

# .iloc
df.iloc[:, [0, 1]]

df.iloc[:, 0:2]

"""## Rows

Selecting rows is easy if you know how to select columns. You have two options:

* With `.loc[]` (by name or tag)
* With `.iloc[]` (by position)

Selecting a single row returns a pandas Series (the 1-dimensional object that pandas has):
"""

df.loc[0]

"""With `.loc[]`, rows are selected by its index name:"""

df.loc[0:3]

"""If we change the index and set it to the `id` column, now the first rows can not be selected the same way:"""

df.set_index('id', inplace=True)
# inplace = True is the same as doing df = df.set_index('id'), i.e. it modifies the dataframe

df.loc[0:3]

df.head(2)

# select the first observation with the .loc() method
df.loc[1119110]

"""With the `iloc[]` method we don't need to know the row names to select rows at a certain position:"""

df.iloc[0:3]

# select the last observation with the method .iloc[]
df.iloc[-1]
# df.tail(1)

df.loc[[1119111,1119112,1119113]]

"""Again, using `:` you can select all elements between the two that you specified:"""

df.loc[1119111:1119113]

df.reset_index(inplace=True)

"""Indexing can get tricky sometimes, it's ok to take some time to get used to the methods we presented, and it's ok to have some trouble selecting the rows and columns you need. For an exhaustive guide on Pandas indexing, check out this link: https://pandas.pydata.org/docs/user_guide/indexing.html#indexing

## Drop and Filter data

The `.drop()` method allows us to delete the rows or columns that we indicate.

**Note:** Again, if we want to directly apply the changes to the original dataframe, we need to indicate `inplace = True`. Otherwise, we are getting as an output just a "view" of how the dataframe looks like after the drop, but the original dataframe remains untouched.
"""

# droping the column "unit_price"
# axis=1 means we want to drop a column, not a row (for rows, axis=0)
df.drop(['unit_price'], axis=1)

"""How to filter information on a dataframe."""

# rows with product quantity larger than 100
df.loc[df['product_quantity'] > 100, :]

"""The `.query()` method can be useful for this purpose, as it resembles SQL syntax. Note that it works only when the column values do not contain blank spaces. You can use any **Python Comparison Operators** you want inside the query method (find more information on this [link](https://www.w3schools.com/python/python_operators.asp))."""

df.query('product_quantity > 100')

"""The `isin()` method is very useful to find rows that match any of the values you have in a list. For example, here we are searching for rows where its `sku` matches any of the 2 sku's we listed:"""

# find out a column that contains a list
df['sku'].isin(['JBL0104', 'ADN0039'])

"""This expression can be used inside of `[]` or `.loc[]` to filter the rows that have a `True` value. This is called "boolean indexing" and it is really useful:"""

my_products = df['sku'].isin(['JBL0104', 'ADN0039'])

df.loc[my_products, :]

"""Pandas compresses large outputs in Colab/Jupyter Notebooks. If you want to see more rows, you can change the options:"""

pd.options.display.max_rows = 100
# after running this cell, run the code above again and you will see all the rows.

"""For a complete list of all settings and options that can be tweaked, check out this: https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html

#### Modifying a dataframe & the `.copy()` method

Let's take a small chunk of data from our dataframe:
"""

sample = df.iloc[:3,:]
sample

"""Now we pick a single cell from the sample we took and we assign a new value to it. A warning already tells us that this is a risky thing to do:"""

# we change the "unit_price" of the first row:
sample.iloc[0,4] = 'NEW VALUE HERE'

"""We can see the new value on the `sample` we took:"""

sample

"""..and, maybe to your surprise, we can see that the new value is also present on the original `df`!"""

df.head()

"""When you take a chunk of data using `.loc[]` or `iloc[]` and assign it to a new object, the new object is just a "tag" pointing to the very same data as the original dataframe points to. We can avoid this using the method `.copy()`"""

url = 'https://drive.google.com/file/d/14Y7g5ITyf6LMyPoKc9wr010V9StaCUux/view?usp=sharing' # orderlines.csv
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
df = pd.read_csv(path)

sample = df.iloc[:3,:].copy()
sample.iloc[0,4] = 'NEW VALUE HERE'
sample

df.head(3)

"""As you can see, now it has not been modified.

# CHALLENGES

1. How many different unit prices does the product with the sku JBL0104 have? Combine a pandas filter method with the method `.nunique()`.
"""

df2=df.query("sku == 'JBL0104'").nunique()
df2['unit_price']
#df.head()

df2 = df.loc[df['sku'] == ''JBL0104''].nunique()

"""2. List the (unique) items that were sold in the order with the id_order 385921."""

# code here
df2 = df.loc[df['id_order'] == '385921']
df2['product_id']
df.head()

"""3. Consider the products with the sku's APP2431 and APP2348. Find out in how

many orders were they present.
"""

# code here
df2 = df.loc[]
df2 = df.query("sku == 'APP2431' & sku == 'APP2348'")
df['id_order']

"""4. Create a new dataframe, `df_50`, with all the rows that have a product quantity higher than 500 and only the columns `id`, id `order`, `product_quantity` and `sku`. Be sure to use the method `.copy()`. Once the new dataframe is created, modify the column 'product_quantity' to 'quantity', and 'sku' to 'product_code'. To do so, you can use the method `.rename()` or assign a list of new names to the attribute `.columns`."""

# code here

"""5. Filter all the order lines where the product with the sku XDO0047 has appeared. Sort the rows by their product quantity using the [`.sort_values()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html) method from pandas, in a DESCENDING order. Then look at the main descriptive information of this results with the method `.describe()`."""

# code here

"""Explore the other dataframes orders, brands, products"""

import pandas as pd


# orders.csv
url = 'https://drive.google.com/file/d/1BLEHcP-9fm9Rv7A01H3co2XBMnSr66YC/view?usp=sharing' 
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
orders = pd.read_csv(path)

# brands.csv
url = 'https://drive.google.com/file/d/1BrNrIY0F1LbyXtyaQygUBXVxQGB3JBqx/view?usp=sharing' 
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
brands = pd.read_csv(path)

# products.csv
url = 'https://drive.google.com/file/d/1UfsHI80cpQqGfsH2g4T4Tsw8cWayOfzC/view?usp=sharing' 
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
products = pd.read_csv(path)