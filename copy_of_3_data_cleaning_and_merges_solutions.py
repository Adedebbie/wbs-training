# -*- coding: utf-8 -*-
"""Copy of 3-data-cleaning-and-merges-solutions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u6LpEHElKuL4B8P4tdziQkTtOnyCQgUM

# Data cleaning and merging dataframes

## Loading multiple datasets

### Google way
"""

import pandas as pd

# orderlines.csv
url = 'https://drive.google.com/file/d/14Y7g5ITyf6LMyPoKc9wr010V9StaCUux/view?usp=sharing' 
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
orderlines = pd.read_csv(path)

# orders.csv
url = 'https://drive.google.com/file/d/1BLEHcP-9fm9Rv7A01H3co2XBMnSr66YC/view?usp=sharing' 
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
orders = pd.read_csv(path)

# brands.csv
url = 'https://drive.google.com/file/d/1BrNrIY0F1LbyXtyaQygUBXVxQGB3JBqx/view?usp=sharing' 
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
brands = pd.read_csv(path)

# products.csv
url = 'https://drive.google.com/file/d/1UfsHI80cpQqGfsH2g4T4Tsw8cWayOfzC/view?usp=sharing' 
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
products = pd.read_csv(path)

df_list = [orderlines, orders, brands, products]
files = ['orderlines','orders','brands','products']

orderlines.head()

orders.head()

brands.head()

products.head()

#products_unique["prices_clean"]= products_unique.apply(lambda x: products_unique["promo_price_c"] if products_unique["price_c"] * 0.05 > products_unique["promo_price_c"] else products_unique["price_c"])

"""### Classical way

Reading file by file
"""

# import pandas as pd

# path = '../data/eniac/'
# orderlines = pd.read_csv(path + 'orderlines.csv')
# orders = pd.read_csv(path + 'orders.csv')
# brands = pd.read_csv(path + 'brands.csv')
# products = pd.read_csv(path + 'products.csv')

# df_list = [orderlines, orders, brands, products]
# files = ['orderlines','orders','brands','products']

"""### Another way

Using `os` and a loop to read all the files from a directory. It's also possible to read only files with a certain extension (like `.csv`):
"""

# import pandas as pd
# import os
# path = '../data/eniac/'
# path, dirs, files = next(os.walk(path))
# #print(files)

# # remove non-csv files
# for file in files:
#     if not file.endswith("csv"):
#         files.remove(file)

# # create empty list 
# df_list = []

# # append datasets to the list
# for file in files:
#     temp_df = pd.read_csv(path + file, sep=',')
#     df_list.append(temp_df)

# # show results
# products, orders, orderlines, brands = df_list[0], df_list[1], df_list[2], df_list[3]

"""## Data quality

### Missing values
"""

# we can check missing values column
orderlines.isna().sum()

# or for the whole dataframe
orderlines.isna().sum().sum()

# the .info() method also tells us the "Non-Null Count" for each column
orderlines.info()

orders.info()

brands.info()

products.info()

"""### Duplicates

The presence of duplicate rows is generally a sign that the data is not clean, and will deserve further exploration.
"""

orderlines.duplicated().sum()

orders.duplicated().sum()

brands.duplicated().sum()

products.duplicated().sum()

"""### Data cleanliness - initial assessment

Based on our initial exploration, we know we will need to deal with some missing values. The biggest issue so far are the duplicates on the `products` DataFrame. Here are some aspects that we will need to fix or, at least, explore further:

* **products**: 
    * `price` and `promo_price` are loaded as objects. They should be floats. 
    * Missing values: 
        * `description`: 7 missing values. Maybe that could be inferred from the product name?
        * `price`: the missing values could be filled from the `orderlines` dataset. But first we will need to clean it. 
    * Duplicates: a total of 8746 duplicates seems to indicate this DataFrame has been seriously corrupted.
    
* **orders**: 
    * `created_date` should have a date data type. Then, it would be a good quality check to see if the created date for `orders` matches with the created dates for `orderlines`. 
    
* **orderlines**: 
    * `unit_price` has to be a float, something is wrong there. 
    * `date` has to be transformed to a date data type. Then, as we said, check that it matches with the `orders` dataset. 
    
* **brands**: looks fine. 

Where do we have to start? 

1. **Data consistency:** Since `orders` and `orderlines` seem very crucial to the analysis, we will start by cleaning them and checking that the information present in both DataFrames match.

2. **The "products mess":** This file seems to have many issues. We will leave it out for now and perform a proper exploration later to understand better what's going on there.

## Cleaning orders

The data consistency check we will do with `orderlines` will involve two steps: 

* the initial and last dates of the orders should be the same
* the sum of `total_paid` on both datasets should be the same

Let's start by transforming the `created_date` of the orders DataFrame and looking for its earliest and latest values:
"""

# change date datatype
orders['created_date'] = pd.to_datetime(orders['created_date'])

# earliest value
min(orders['created_date'])

# latest value
max(orders['created_date'])

"""Now we will look at the overall sum of `total_paid` for the orders table:"""

sum(orders['total_paid'])

"""Why do you think the result of the sum is a nan (not a number)?"""

orders.total_paid.isna().sum()

"""There are missing values! We can explore them and see how they are all "Pending" orders:"""

orders.loc[orders['total_paid'].isna()]

"""Since these orders are only a tiny fraction and there's a valid reason why the `total_paid` value is missing, we will simply exclude them from the dataset:"""

orders.dropna(inplace=True)

"""Now the dataset is clean. And the total paid is: """

orders['total_paid'].sum()

"""## Cleaning orderlines

Following our data consistency check, will now gather in the orderlines DataFrame the same information we got from orders:

* the initial and last dates
* the sum of `total_paid`


First let's transform our date time:
"""

orderlines['date'] = pd.to_datetime(orderlines['date'])

min(orderlines['date'])
# orders: Timestamp('2017-01-01 00:07:19')

max(orderlines['date'])
# orders: Timestamp('2018-03-14 13:58:36')

"""**It's a match!**

Now let's check the `total_paid` for orderlines. It's not going to be as easy as with the orders DataFrame, considering the structure of orderlines:

"""

orderlines.head(3)

"""
To get this value, we will have to calculate a new column, total price for each row. It would be `product_quantity` * `unit_price`. This operation will require that both columns have a numeric data type:"""

orderlines.dtypes

products.head()

"""...it's not the case right now, so we will have to transform the `unit_price` to a numeric data type. """

# uncomment the line of code below and read the error it produces:
# orderlines['product_quantity'] * pd.to_numeric(orderlines['unit_price'])

"""While trying to transform this column to numeric an error appears. From the error message, 

> "Unable to parse string '1.137.99' at position 6"

it seems that our dataset has some problems with the thousands separators: they were encoded as dots, and Python & pandas only admit one dot per number: the _decimal_ separator!

Lesson learned: do not use thousand separators in databases / statistical software / programming languages! Sadly, it's too late for us, and we will have to deal with the issue.

There are many ways to approach this problem. The first thing we will do is to count how many dots appear for each `unit_price` value, using string operations. If there are two or more dots a value, we will consider it corrupted â€”and either try to fix it, or remove it completely.
"""

# we create a copy of the dataset
ol_temp = orderlines.copy()

# create a new column with the amount of dots in the unit_price column
ol_temp['dots'] = orderlines['unit_price'].str.count('\.') # the backslash 'escapes' the special meaning of '.' in string operations

# show the rows with more than one dot
ol_temp.query('dots > 1')

"""Our theory about the thousands separators is confirmed. How can we solve this problem? 

Let's remove all the dots for all the `unit_price`, and then add a dot before the last 2 digits to all the rows. Then we will transform it into numeric values.

##### step 1: remove all dots
a) A "corrupted" price like `1.137.99`	will become `113799`

b) A correct price like `37.99`	will become `3799`

##### step 2: add dots two digits before the end of the number
a) The "corrupted" price will go from `113799` to `1137.99`

b) The correct price will go from `3799` back to `37.99`.
"""

# step 1: to remove the dots, we replace them with... nothing
orderlines = orderlines.assign(unit_price_nd = orderlines['unit_price'].str.replace('\.','', regex=True))
orderlines.head()

# step 2.1: we first separate all numbers between the part that goes before the
# decimal point (integers) and the part that goes afterwards (decimals)
orderlines['integers'] = orderlines['unit_price_nd'].str[:-2]
orderlines['decimals'] = orderlines['unit_price_nd'].str[-2:]
orderlines.head()

# we create a copy of the dataset
ol_temp1 = products.copy()

# create a new column with the amount of dots in the unit_price column
ol_temp1['dots'] = products['promo_price'].str.count('\.') # the backslash 'escapes' the special meaning of '.' in string operations

# show the rows with more than one dot
ol_temp1.query('dots == 0')

# step 1: to remove the dots, we replace them with... nothing
products = products.assign(promo_price_nd = products['promo_price'].str.replace('\.','', regex=True))
products.head()

#products.price.to_numeric()
#products["price_test"] = pd.to_numeric(products["price"])
#products["price_test"][:10]

products['integers'] = products['promo_price_nd'].str[:-2]
products['decimals'] = products['promo_price_nd'].str[-2:]
products.head()

products['new_promo_price'] = products['integers'] + '.' + products['decimals']
products.head()

# step 2.2: we now concatenate those two parts of the number, with a dot in between
orderlines['new_unit_price'] = orderlines['integers'] + '.' + orderlines['decimals']
orderlines.head()

"""We will now try again to convert this column to a numeric data type:"""

orderlines['unit_price'] = pd.to_numeric(orderlines['new_unit_price'])
orderlines.info()

"""Data cleaning done! 

Back to data consistency: Now it is time to multiply `product_quantity` and `unit price`, sum all the rows and check whether the value matches the sum of the `total_paid` from the orders DataFrame: 
"""

# drop 'auxiliary' columns
orderlines.drop(['unit_price_nd','decimals','integers','new_unit_price'], axis=1, inplace=True)

# create a new column "total_price" by multiplying product_quantity with unit_price
orderlines['total_price'] = orderlines['product_quantity'] * orderlines['unit_price']

# sum of the new column "total_price":
sum(orderlines['total_price'])

"""Sadly, it does not match exactly the sum of `total_paid` from orders:"""

orders['total_paid'].sum()

"""The mismatch is about 383 thousand dollars, a non-negligible amount of money:"""

sum(orderlines['total_price']) - orders['total_paid'].sum()

"""How can we figure out where the difference comes from?

## Matching `orders` and `orderlines`

It is possible that some orders exist in one dataset but not in the other one. This would be a potential source for this price mismatch. Let's find out!

We first create a new column in the `orderlines` dataset using `assign`. We also use `isin()` to create a boolen value (True/False) that checks whether the `id_order` is present in the `orders` dataset:
"""

orderlines.assign(check_orders = orderlines['id_order'].isin(orders['order_id']))

"""Then, using `.query` we select rows where the value in this new column is `False`:"""

(
orderlines
    .assign(check_orders = orderlines['id_order'].isin(orders['order_id']))
    .query("check_orders==False")
)

"""It looks like 240 rows in `orderlines` come from orders not present in the `orders` dataset. This is quite inconsistent, since the `orders` dataset should be the one and only source of truth for orders: if an order is not there, it should not exist. We will definitely report this anomaly, but for now, let's just remove those "ghost" orders:"""

orderlines = (orderlines
              .assign(check_orders = orderlines['id_order'].isin(orders['order_id']))
              .query("check_orders==True"))

"""Now let's look at this problem in the opposite direction: are there orders in the `orders` dataset not present in `orderlines`?"""

(orders
 .assign(check_orders = orders['order_id'].isin(orderlines['id_order']))
 .query("check_orders==False"))

"""There are more than 22000 orders in the `orders` dataset that are not present on the `orderlines` dataset!!! We can try to find out why by looking at the state of these orders:"""

(orders
 .assign(check_orders = orders['order_id'].isin(orderlines['id_order']))
 .query("check_orders==False")
 ['state'].value_counts())

"""It looks like most of them are orders that were not fully completed: the products were left in the shopping basket or the order was "placed" but maybe not paid (hence the state "Place Order". Some of them were "Completed", though. 

This will require further research, and we might have to come back to these orders if we have to explore consumer behaviour (e.g. why are orders left in the shopping basket?), but for now, for the sake of data consistency, let's drop all of these unmatched orders:
"""

orders = (orders
          .assign(check_orders = orders['order_id'].isin(orderlines['id_order']))
          .query("check_orders==True")
         )

"""Let's now check again if the total paid matches:"""

orders['total_paid'].sum()

orderlines['total_price'].sum()

"""STILL NOT MATCHING!!! And actually, the difference got larger. This is outrageous. Let's keep exploring."""

orderlines['total_price'].sum() - orders['total_paid'].sum()

"""## Solving the price mismatch

Let's merge both datasets and compare, order by order, the `total_price`. We will call this new merged dataset `orders_info`.

*Note: Remember that the `orderlines` dataset contains one row per product bought: an order where 3 different products were purchased will result in 3 rows there. Therefore, to merge `orderlines` with `orders`, we have to group `orderlines` by its `id_order` and aggregate it by taking the sum of the `total_price`.
"""

orders_info = (
orderlines
    .groupby('id_order')
    .agg({'total_price':'sum'})
    .merge(orders, how='left', left_on='id_order', right_on='order_id')
    .copy()
)
orders_info


###
orderlines_agg = orderlines\
    .groupby('id_order')\
    .agg({'total_price':'sum'})

orders_info = orderlines_agg.merge(orders, how='left', left_on='id_order', right_on='order_id').copy()
orders_info

"""Now that the `total` from both datasets is in the same dataframe, we can create a new column with the difference:"""

orders_info['price_difference'] = orders_info['total_price'] - orders_info['total_paid']
orders_info.sort_values('price_difference').tail(30)

orders_info['price_difference'].describe()

"""Looks like the maximum and minimum price differences are huge: some orders are really corrupted. But we also see from the mean and the quartiles that the price difference is neglegible for most orders.

## Challenge: Remove outliers

Decide on a criterion for removing orders whenever you cannot trust the price difference between `orders` and `orderlines`. 

Note: this solution does not completely achieve 100% trustable data, but the objective here is to end up with the best possible data within a limited amount of time, which makes a complete revision of the database protocols and the data pipelines is not feasible - and business questions are pending. Documenting and reporting this data cleaning process, including the criterion that you will use for determining what do you consider an "outlier", is key. Not being paralyzed by it is also important!

**Finding Outliers Using the Interquartile Rule**

We can use the interquartile range to identify outliers 
* Determine the data's interquartile range.
* Multiply the interquartile range (IQR) by 1.5 - call this 1.5_iqr
* Add 1.5_iqr to the 75th percentile. Any figure above this is thought to be an outlier.
* Subtract 1.5_iqr from the 25th percentile. Any value below this is thought to be an outlier.

Just keep in mind that the interquartile rule is simply a generalisation, not all situations call for it. In general, you should always check to see if the resulting outliers make sense by reviewing them after conducting your outlier analysis.
"""

# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.quantile.html

# Calculate the 25th & 75th percentiles
Q1 = orders_info['price_difference'].quantile(0.25)
Q3 = orders_info['price_difference'].quantile(0.75)
# Calculate the interquartile range
IQR = Q3-Q1
# filter the DataFrame to include only "non-outliers"
filtered = orders_info.loc[(orders_info['price_difference'] >= (Q1 - 1.5*IQR)) & (orders_info['price_difference'] <= (Q3 + 1.5*IQR)), :]

filtered.head()

filtered.shape

# upper limit
Q3 + 1.5*IQR

# lower limit
Q1 - 1.5*IQR

"""Save the data once you are sure you can trust it!"""

#orderlines.to_csv('orderlines_cl.csv', index=False)
#orders.to_csv('orders_cl.csv', index=False)

#from google.colab import files
#files.download("orderlines_cl.csv")
#files.download("orders_cl.csv")

"""## Challenge: Cleaning products

Now it is time to clean the products dataset. Let's do a quick review of its major problems:
"""

print(products.info(), "\n")
print("Missing values:", products.isna().sum(), "\n")
print("Duplicate rows:", products.duplicated().sum())

"""Looking at this overview, we can see that there are different things that have to be changed: 

* Data types: 
    * `price` should be a float
    * `promo price` should be a float
* Duplicated rows. They have to be removed. 
    * To accomplish this step you can use the method `pd.DataFrame.drop_duplicates()`. Be sure you drop all the duplicates based on the column **sku**, as it is the one you will use to merge with orderlines. 
* Missing values: 
    * Description maybe can be inferred by the name
    * `price`. Is there a way we can extract the information from another table?
    * `type`. Do we need this column for our analysis?
    
These tasks can be accomplished using all the methods you already know.

### Start of the challenge

#### Duplicates
"""

products.drop_duplicates("sku",inplace=True)
products.head(3)

products.shape

products.head()

"""Check if you have a unique description for each sku: """

products.groupby('sku')['desc'].count().sort_values(ascending=False)

"""The product APP1197 is not unique. Let's zoom in: """

products.query('sku == "APP1197"')

products.drop_duplicates('sku').query('sku == "APP1197"')

"""I can drop the duplicates only focusing on the column 'sku': """

products.drop_duplicates('sku', inplace=True)

"""#### Data types"""

#pd.to_numeric(products['price'])

"""It seems this time that the problem in our products prices is different from the one we found on orderlines. How is it possible that we have more than 3 digits? 

Let's look at a random sample in our dataframe to understand more about it: 
"""

products.sample(10)

"""My strategy would be: 

1. Replace missing values with a "special" string. In that case I will put "000.000" as it would represent that this value has a wrong format and has to be changed. This step is done to avoid future problems while cleaning the column price.
2. Create a new column called "price_split" with the values of price split by the `.`
3. Create a new column called "dots_count" counting the number of dots a price contains.
"""

# Replace all missing values by '000.000'
products = products.assign(price = products['price'].fillna('000.000'))

# split the price and add a dot count
products = products.assign(price_split = products['price'].str.split('\.'), 
                           dots_count = products['price'].str.count('\.'))
products.head()

"""Now I need to look at each value in the new column "price_split" and check the following condition: 

If the last element of the value in price split has a length bigger than 2 then add a `True` inside the list `need_check`, else add a `False`. 

Example 1: 

 - Input: price_split = `[[59],[99]]` The last element of the value in price split is `[99]`. The length of this element is 2, as it only has 2 characters. Then we will append a `False` to the list `need_check`. 

Exemple 2: 

- Input: price_split = `[[234],[895]]` The last element of the value in price split is `[895]`. The length of the last element is 3, as it has 3 characters. Then we will append a `True` to the list `need_check`.
"""

need_check = []
len_split = []
for val in products['price_split']:    
    len_split.append(len(val))
    if len(val[-1]) > 2: 
        need_check.append(True)
    else:
        need_check.append(False)

"""We have created two lists: 

* `need_check`: tells us which element of the dataframe has a last element on `price_split` with a length greater than 2. 
* `len_split` : tells us how many elements are inside each `price_split`. 
    * Example 1: `[[59],[99]]` it has a length of 2
    * Example 2: `[59]` it has a length of 1
    * Example 3: `[[654],[998]]` it has a length of 2
    * Example 4: `[[9],[654],[998]]` it has a length of 3
    
Then we can add these elements into our products dataframe to detect these rows: 
"""

(
products
    .assign(need_check = need_check, 
            len_split = len_split)
    .query('need_check==True & len_split > 1 & dots_count != 0')
).head()

"""By defining a set of rules, I can quickly detect the prices that are giving me errors. Those rules are: 

* The length of the last element on the value `price_split` has a length bigger than 2 (This is what the column `need_check` is telling us).
* There is more than 1 element inside the column `price_split` (Information given by the column `len_split`). 
* The number of dots in price is different than 0 (Column `dots_count`). 

The next steps would be to use these conditions to replace these values by missing values. To do it, I am going to use the function from the module `numpy` called `np.where`. It works like an `if else` statement; if a condition is met, then return a value, else, return another value. 
"""

products= products.assign(need_check = need_check, len_split = len_split)
products.loc[(products.need_check==True) & (products.len_split > 1) & (products.dots_count != 0), 'price'] = 'Null'
# errors=coerce sets all none numerical values to NaN
products['price'] = pd.to_numeric(products['price'], errors='coerce')
products.query('need_check==True & len_split > 1 & dots_count != 0').head(10)

"""As you can see, we replaced all the values with wrong prices with missing data. New let's drop all the columns we do not need: """

products.drop(['price_split','need_check','len_split','dots_count'], axis=1, inplace=True)
products["price"].isna().sum()

"""And now is the time to find a strategy to fill these missing values. """

products.info()

"""#### Missing values

As of now we have incremented the number of missing data (we transformed all the wrong prices to missing ones) let's find out a strategy to fill them. 

But first, let's see if they are important in our dataset:
"""

products.assign(row_na = products.isna().sum(axis=1) > 0).query('row_na == False')

prod_na = products.loc[products.isna().sum(axis=1) > 0,:].copy()
prod_na.head()

"""Decide which percentage of the entire dataset these products represent. Always think about how to use your time wisely: 

* Which percentage of the orders contains one of these products? In that case I have to look at orderlines.
"""

orderlines.shape[0]

# Orders with products with missing prices on products dataset
orderlines.loc[orderlines['sku'].isin(prod_na['sku']),:].shape[0] / orderlines.shape[0]

# this is interesting, as for the products data it is much smaller
prod_na.shape[0] / products.shape[0]

"""Which impact do they have on total revenue?"""

orderlines.loc[orderlines['sku'].isin(prod_na['sku']),:]['total_price'].sum() / orderlines['total_price'].sum()

"""With 3% only of the total revenue, but we are considering all the orders, and we are only interested in the completed ones. """

na_skus = prod_na['sku'].tolist()

(
orderlines
    .merge(orders, how='left', left_on='id_order', right_on='order_id')
    .query('state == "Completed" & sku == @na_skus')
    ['total_price'].sum()
) / (
orderlines
    .merge(orders, how='left', left_on='id_order', right_on='order_id')
    .query('state == "Completed"')
    ['total_price'].sum()
)

"""As you can see, the products that are missing only represent 2.9% of the total revenue. 
I will not drop them, as they still have some information which I would like to keep to create categories (sku, item name and item description), so I am going to save them with prices as missing values. But I will add an extra column telling me which is the maximum price for these skus in orderlines.
"""

# find out which sku are missing in products
prod_na_list = (
products
    .loc[products['price'].isna(),'sku']
    .tolist()
)

# find them on orderlines, and aggregate the information to find the maximum value
orderlines_prod_na_max = (
orderlines
    .query("sku == @prod_na_list")
    .groupby('sku') 
    .agg({'unit_price':'max'})
    .rename(columns={'unit_price':'max_price_orderlines'})
    .reset_index()
)
orderlines_prod_na_max[:5]

"""Time to add the column `max_price_orderlines` in our products data:"""

products = (
products
    .merge(orderlines_prod_na_max, how='left',on='sku')
)
products.loc[products['max_price_orderlines'].notna()].sample(10)

"""And now let's store it as a products clean csv into our path: """

#products.drop(['promo_price','in_stock'], axis=1).to_csv('products_cl.csv', index=False)
#files.download("products_cl.csv")

"""## Brands

The brands csv looks fine, so we can work with it.
"""

#brands.to_csv('brands_cl.csv', index=False)
#files.download("brands_cl.csv")